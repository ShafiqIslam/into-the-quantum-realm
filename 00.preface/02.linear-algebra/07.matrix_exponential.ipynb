{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Exponentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notion of a matrix exponential is a very specific yet extremely important concept. We often see unitary transformations in the form:\n",
    "\n",
    "$$U \\ = \\ e^{i\\gamma H} \\ ,$$\n",
    "\n",
    "where $H$ is some Hermitian matrix and $\\gamma$ is some real number. It is fairly simple to prove that all matrices of this form are unitary. Taking the conjugate transpose of $U$, we get:\n",
    "\n",
    "$$U^{\\dagger} \\ = \\ \\Big( e^{i\\gamma H} \\Big)^{\\dagger} \\ = \\ e^{-i \\gamma H^{\\dagger}}$$\n",
    "\n",
    "But since $H$ is Hermitian, we know that $H^{\\dagger} \\ = \\ H$, thus:\n",
    "\n",
    "$$e^{-i \\gamma H^{\\dagger}} \\ = \\ e^{-i \\gamma H} \\ \\Rightarrow \\ U^{\\dagger} U \\ = \\ e^{-i \\gamma H} e^{i\\gamma H} \\ = \\ \\mathbb{I}$$\n",
    "\n",
    "You may wonder why a matrix inside of an exponential can still be considered a matrix.  The answer becomes clearer when we expand our exponential function as a Taylor series. Recall from calculus that a Taylor series is essentially a way to write any function as an infinite-degree polynomial, and the main idea is to choose the terms of the polynomial and center it at some point $x_0$ lying on the function we are trying to transform into the polynomial, such that the zeroth, first, second, third, etc. derivative is the same for both the original function and the polynomial. Thus, we write our Taylor series in the form:\n",
    "\n",
    "$$g(x) \\ = \\ \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ f^{(n)}(x_0) \\ \\frac{(x \\ - \\ x_0)^n}{n!} \\ ,$$\n",
    "\n",
    "where $g(x)$ is the polynomial, $f(x)$ is the original function, $f^{(n)}$ is the $n$-th derivative of $f$, and $x_0$ is the point at which we center the function. Since we are not approximating, $x_0$ doesn't matter, so for simplicity, we choose $x_0 \\ = \\ 0$, and the Taylor series becomes a Maclaurin series:\n",
    "\n",
    "$$g(x) \\ = \\ \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ f^{(n)}(0) \\ \\frac{x^n}{n!}$$\n",
    "\n",
    "If we choose $f(x) \\ = \\ e^x$, we can create an equivalent polynomial using the Maclaurin series. Since the derivative of $e^x$ is simply $e^x$, and evidently, $e^0 \\ = \\ 1$, we get:\n",
    "\n",
    "$$g(x) \\ = \\ \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ \\frac{x^n}{n!} \\ = \\ e^x$$\n",
    "\n",
    "Thus, for some matrix, $i \\gamma H$, we get:\n",
    "\n",
    "$$e^{i \\gamma H} \\ = \\ \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ \\frac{(i \\gamma H)^n}{n!}$$\n",
    "\n",
    "Therefore, the exponential of a matrix is a matrix. It is an infinite sum of powers of matrices, which admittedly looks overly complex...but the point here is that the matrix exponential is indeed a matrix. \n",
    "\n",
    "We are now in a position to demonstrate a very important fact: if we have some matrix $B$ such that $B^2 \\ = \\ \\mathbb{I}$ (this is called an **involutory matrix**), then:\n",
    "\n",
    "$$e^{i \\gamma B} \\ = \\ \\cos(\\gamma) \\mathbb{I} \\ + \\ i \\sin(\\gamma) B$$\n",
    "\n",
    "We start with the Maclaurin series:\n",
    "\n",
    "$$e^{i \\gamma B} \\ = \\ \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ \\frac{(i \\gamma B)^n}{n!}$$\n",
    "\n",
    "Notice that we can split the summation into an imaginary part and a real part, based on whether $n$ is even or odd in each term of the sum:\n",
    "\n",
    "$$\\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ \\frac{(i \\gamma B)^n}{n!} \\ = \\ \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ \\frac{(-1)^n \\gamma^{2n} B^{2n}}{(2n)!} \\ + \\ i \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\frac{(-1)^n \\gamma^{2n + 1} B^{2n + 1}}{(2n + 1)!}$$\n",
    "\n",
    "Now, let us find the Maclaurin series for both $\\sin x$ and $\\cos x$. We'll start with $f(x) \\ = \\ \\sin x$:\n",
    "\n",
    "$$\\sin x \\ = \\ \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ f^{n}(0) \\frac{x^n}{n!}$$\n",
    "\n",
    "The derivative of $\\sin x$ is **cyclical** in a sense (each arrow represents taking the derivative of the previous function):\n",
    "\n",
    "$$\\sin x \\ \\rightarrow \\ \\cos x \\ \\rightarrow \\ -\\sin x \\ \\rightarrow \\ -\\cos x \\ \\rightarrow \\ \\sin x$$\n",
    "\n",
    "Since $\\sin (0) \\ = \\ 0$ and $\\cos (0) \\ = \\ 1$, all terms with even $n$ become $0$, and we get:\n",
    "\n",
    "$$\\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ f^{n}(0) \\frac{x^n}{n!} \\ = \\ \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ \\frac{(-1)^n x^{2n \\ + \\ 1}}{(2n \\ + \\ 1)!}$$\n",
    "\n",
    "This looks similar to the odd term of our original equation. In fact, if we let $x \\ = \\ \\gamma B$, they are exactly the same. We follow a process that is almost identical to show that the even terms are the same as the Maclaurin series for $f(x) \\ = \\ \\cos x$:\n",
    "\n",
    "$$\\cos x \\ = \\ \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ f^{n}(0) \\frac{x^n}{n!}$$\n",
    "\n",
    "$$\\Rightarrow \\ \\cos x \\ \\rightarrow \\ -\\sin x \\ \\rightarrow \\ -\\cos x \\ \\rightarrow \\ \\sin x \\ \\rightarrow \\ \\cos x$$\n",
    "\n",
    "$$\\Rightarrow \\ \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ f^{n}(0) \\frac{x^n}{n!} \\ = \\ \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ \\frac{(-1)^n x^{2n}}{(2n)!}$$\n",
    "\n",
    "Let us go back to the original equation. Recall that $B^2 \\ = \\ \\mathbb{I}$. For any $n$, we have:\n",
    "\n",
    "$$B^{2n} \\ = \\ \\big( B^2 \\Big)^n \\ = \\ \\mathbb{I}^n \\ = \\ \\mathbb{I}$$\n",
    "\n",
    "$$B^{2n \\ + \\ 1} \\ = \\ B \\ \\big( B^2 \\Big)^n \\ = \\ B \\ \\mathbb{I}^n \\ = \\ B \\ \\mathbb{I} \\ = \\ B$$\n",
    "\n",
    "Substituting in this new information, we get:\n",
    "\n",
    "$$\\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ \\frac{(-1)^n \\gamma^{2n} B^{2n}}{(2n)!} \\ + \\ i \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\frac{(-1)^n \\gamma^{2n + 1} B^{2n + 1}}{(2n + 1)!} \\ = \\ \\mathbb{I} \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ \\frac{(-1)^n \\gamma^{2n}}{(2n)!} \\ + \\ i B \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\frac{(-1)^n \\gamma^{2n + 1}}{(2n + 1)!} \\ = \\ \\cos (\\gamma) \\mathbb{I} \\ + \\ i \\sin (\\gamma) B$$\n",
    "\n",
    "This fact is extremely useful in quantum computation. Consider the Pauli matrices:\n",
    "\n",
    "$$\\sigma_x \\ = \\ \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$$\n",
    "\n",
    "$$\\sigma_y \\ = \\ \\begin{pmatrix} 0 & -i \\\\ i & 0 \\end{pmatrix}$$\n",
    "\n",
    "$$\\sigma_z \\ = \\ \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}$$\n",
    "\n",
    "These matrices are among the fundamental \"quantum gates\" used to manipulate qubits. These operations are not only unitary, they are also **Hermitian** and **Involutory**. This means that a matrix of the form $e^{i \\gamma \\sigma_k} \\ k \\ \\in \\ \\{x, \\ y, \\ z\\}$ is not only a valid unitary matrix that can act upon a quantum state vector (a qubit), but it can be expressed using the sine-cosine relationship that we just proved. This is very powerful, and is seen throughout quantum computational theory, as gates of this type are used all the time.\n",
    "\n",
    "One last important fact about matrix exponentials: if we have some matrix $M$, with eigenvectors $|v\\rangle$ and corresponding eigenvalues $\\lambda$, then:\n",
    "\n",
    "$$e^{M} |v\\rangle \\ = \\ e^\\lambda |v\\rangle$$\n",
    "\n",
    "This one is much more straightforward to prove:\n",
    "\n",
    "$$e^M |v\\rangle \\ = \\ \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ \\frac{M^n |v\\rangle}{n!} \\ = \\ \\displaystyle\\sum_{n \\ = \\ 0}^{\\infty} \\ \\frac{\\lambda^n |v\\rangle}{n!} \\ = \\ e^\\lambda |v\\rangle$$\n",
    "\n",
    "This fact is also very useful. When creating quantum circuits that simulate a certain Hamiltonian (especially for variational circuits), we frequently use gates of the form $e^{i \\gamma \\sigma_z}$. Since $|0\\rangle$ and $|1\\rangle$ are eigenvectors of $\\sigma_z$, we can easily determine mathematically that $e^{i \\gamma \\sigma_z}$ will add a phase of $e^{i \\gamma}$ to $|0\\rangle$, and will add a phase of $e^{-i\\gamma}$ to $|1\\rangle$. We can then construct this gate in terms of $CNOT$ and phase/rotation gates fairly easily, as we know the mathematical outcome of the gate on each of the computational basis states.\n",
    "\n",
    "This fact doesn't only apply to exponentials of the $\\sigma_z$ gate. For example, we can determine the outcome of a gate of the form $e^{i \\gamma \\sigma_x}$ on the eigenvectors of $\\sigma_x$, $(|0\\rangle \\ + \\ |1\\rangle)/\\sqrt{2}$ and $(|0\\rangle \\ - \\ |1\\rangle)/\\sqrt{2}$. The same applies to exponentials of the $\\sigma_y$ matrix."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
